{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIAkNm5OF8Ca"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torchvision \n",
        "# import torchvision.transforms as transforms\n",
        "# import torch.utils.data as data\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import urllib.request as urll\n",
        "\n",
        "# torch.__version__\n",
        "\n",
        "# # ______________________________________________________________________________\n",
        "# # QUESTION 1.1.1 \n",
        "\n",
        "# # Here we used the urllib.request library\n",
        "# # https://docs.python.org/3/library/urllib.request.html\n",
        "\n",
        "# # URL as provided \n",
        "# booktext2 = urll.urlopen(\"https://www.gutenberg.org/files/49010/49010-0.txt\")\n",
        "# booktext = urll.urlopen(\"https://www.gutenberg.org/files/49010/49010-0.txt\")\n",
        "\n",
        "# booktext_formatted = str(booktext.read(), 'utf-8')\n",
        "# booktext_string = booktext2.read()\n",
        "# print(booktext_string)\n",
        "# print(booktext_formatted)\n",
        "\n",
        "# # number_of_characters\n",
        "# nrOfChar = booktext_formatted.strip()\n",
        "# print(\"Total number of characters\", len(booktext_formatted))\n",
        "\n",
        "# # number_of_unique_characters\n",
        "# myset = set()\n",
        "# for elem in booktext_formatted:\n",
        "#   myset.add(elem)\n",
        "\n",
        "# print(myset)\n",
        "# print(len(myset))\n",
        "  \n",
        "# # Number_of_lines_in_file\n",
        "# # done in terminal\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "T9dsYMbYVGz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "\n",
        "    def __init__(self, pad_token=\"<pad>\", unk_token='<unk>'):\n",
        "        self.id_to_string = {}\n",
        "        self.string_to_id = {}\n",
        "        \n",
        "        # add the default pad token\n",
        "        self.id_to_string[0] = pad_token\n",
        "        self.string_to_id[pad_token] = 0\n",
        "        \n",
        "        # add the default unknown token\n",
        "        self.id_to_string[1] = unk_token\n",
        "        self.string_to_id[unk_token] = 1        \n",
        "        \n",
        "        # shortcut access\n",
        "        self.pad_id = 0\n",
        "        self.unk_id = 1\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.id_to_string)\n",
        "\n",
        "    def add_new_word(self, string):\n",
        "        self.string_to_id[string] = len(self.string_to_id)\n",
        "        self.id_to_string[len(self.id_to_string)] = string\n",
        "\n",
        "    # Given a string, return ID\n",
        "    def get_idx(self, string, extend_vocab=False):\n",
        "        if string in self.string_to_id:\n",
        "            return self.string_to_id[string]\n",
        "        elif extend_vocab:  # add the new word\n",
        "            self.add_new_word(string)\n",
        "            return self.string_to_id[string]\n",
        "        else:\n",
        "            return self.unk_id\n",
        "\n",
        "\n",
        "# Read the raw txt file and generate a 1D PyTorch tensor\n",
        "# containing the whole text mapped to sequence of token IDs, and a vocab object.\n",
        "class TextData:\n",
        "\n",
        "    def __init__(self, file_path, vocab=None, extend_vocab=True, device='cuda'):\n",
        "        self.data, self.vocab = self.text_to_data(file_path, vocab, extend_vocab, device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def text_to_data(self, text_file, vocab, extend_vocab, device):\n",
        "        \"\"\"Read a raw text file and create its tensor and the vocab.\n",
        "\n",
        "        Args:\n",
        "          text_file: a path to a raw text file.\n",
        "          vocab: a Vocab object\n",
        "          extend_vocab: bool, if True extend the vocab\n",
        "          device: device\n",
        "\n",
        "        Returns:\n",
        "          Tensor representing the input text, vocab file\n",
        "\n",
        "        \"\"\"\n",
        "        assert os.path.exists(text_file)\n",
        "        if vocab is None:\n",
        "            vocab = Vocabulary()\n",
        "\n",
        "        data_list = []\n",
        "\n",
        "        # Construct data\n",
        "        full_text = []\n",
        "        print(f\"Reading text file from: {text_file}\")\n",
        "        with open(text_file, 'r') as text:\n",
        "            for line in text:\n",
        "                tokens = list(line)\n",
        "                for token in tokens:\n",
        "                    # get index will extend the vocab if the input\n",
        "                    # token is not yet part of the text.\n",
        "                    full_text.append(vocab.get_idx(token, extend_vocab=extend_vocab))\n",
        "\n",
        "        # convert to tensor\n",
        "        data = torch.tensor(full_text, device=device, dtype=torch.int64)\n",
        "        print(\"Done.\")\n",
        "\n",
        "        return data, vocab\n",
        "    \n",
        "\n",
        "# Since there is no need for schuffling the data, we just have to split\n",
        "# the text data according to the batch size and bptt length.\n",
        "# The input to be fed to the model will be batch[:-1]\n",
        "# The target to be used for the loss will be batch[1:]\n",
        "class DataBatches:\n",
        "\n",
        "    def __init__(self, data, bsz, bptt_len, pad_id):\n",
        "        self.batches = self.create_batch(data, bsz, bptt_len, pad_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.batches[idx]\n",
        "\n",
        "    def create_batch(self, input_data, bsz, bptt_len, pad_id):\n",
        "        \"\"\"Create batches from a TextData object .\n",
        "\n",
        "        Args:\n",
        "          input_data: a TextData object.\n",
        "          bsz: int, batch size\n",
        "          bptt_len: int, bptt length\n",
        "          pad_id: int, ID of the padding token\n",
        "\n",
        "        Returns:\n",
        "          List of tensors representing batches\n",
        "\n",
        "        \"\"\"\n",
        "        batches = []  # each element in `batches` is (len, B) tensor\n",
        "        text_len = len(input_data)\n",
        "        segment_len = text_len // bsz + 1\n",
        "\n",
        "        # Question: Explain the next two lines!\n",
        "        padded = input_data.data.new_full((segment_len * bsz,), pad_id)\n",
        "        # print(\"Input data is: \", input_data.data)\n",
        "        # print(\"Input shape is\", input_data.data.shape)\n",
        "        # print(\"Padded input data is:\", padded)\n",
        "        # print(\"Padded shape is\", padded.shape)\n",
        "        padded[:text_len] = input_data.data\n",
        "        # print(\"Padded2 is data is:\", padded[:text_len])\n",
        "        # print(\"Padded2 shape is\", padded[:text_len].shape)\n",
        "        # print(\"Padded3 is data is:\", padded)\n",
        "        # print(\"Padded3 shape is\", padded.shape)\n",
        "        padded = padded.view(bsz, segment_len).t()\n",
        "        num_batches = segment_len // bptt_len + 1\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            # Prepare batches such that the last symbol of the current batch\n",
        "            # is the first symbol of the next batch.\n",
        "            if i == 0:\n",
        "                # Append a dummy start symbol using pad token\n",
        "                batch = torch.cat(\n",
        "                    [padded.new_full((1, bsz), pad_id),\n",
        "                     padded[i * bptt_len:(i + 1) * bptt_len]], dim=0)\n",
        "                # print(\"1.2.6\", padded[i * bptt_len:(i + 1) * bptt_len].shape)\n",
        "                # print(\"1.2.6 bsz\", bsz)\n",
        "                # print(\"1.2.6 bptt_len\", bptt_len)\n",
        "                batches.append(batch)\n",
        "            else:\n",
        "                batches.append(padded[i * bptt_len - 1:(i + 1) * bptt_len])\n",
        "                # print(\"1.2.7\", padded[i * bptt_len - 1:(i + 1) * bptt_len].shape)\n",
        "                # print(\"1.2.7 bsz\", bsz)\n",
        "                # print(\"1.2.7 bptt_len\", bptt_len)\n",
        "\n",
        "        return batches"
      ],
      "metadata": {
        "id": "OtJAaQcZklnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downlaod the text\n",
        "# Make sure to go to the link and check how the text looks like.\n",
        "!wget http://www.gutenberg.org/files/49010/49010-0.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TShE1Iv4kngk",
        "outputId": "af906540-e81f-4a5f-9b7a-9dc6033cc7d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-26 16:54:53--  http://www.gutenberg.org/files/49010/49010-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/49010/49010-0.txt [following]\n",
            "--2022-11-26 16:54:54--  https://www.gutenberg.org/files/49010/49010-0.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 185303 (181K) [text/plain]\n",
            "Saving to: ‘49010-0.txt.18’\n",
            "\n",
            "49010-0.txt.18      100%[===================>] 180.96K   265KB/s    in 0.7s    \n",
            "\n",
            "2022-11-26 16:54:56 (265 KB/s) - ‘49010-0.txt.18’ saved [185303/185303]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is for Colab. Adapt the path if needed.\n",
        "text_path = \"/content/49010-0.txt\""
      ],
      "metadata": {
        "id": "ADpqwcF_ko9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda'\n",
        "\n",
        "batch_size = 32\n",
        "bptt_len = 64\n",
        "\n",
        "my_data = TextData(text_path, device=DEVICE)\n",
        "batches = DataBatches(my_data, batch_size, bptt_len, pad_id=0)"
      ],
      "metadata": {
        "id": "vlV6n6TYkq1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a0bce6-d183-461d-9c6e-497a98bf729e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading text file from: /content/49010-0.txt\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "NFxn0nQimNv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN based language model\n",
        "class LSTModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, emb_dim, hidden_dim, num_layers):\n",
        "        \"\"\"Parameters:\n",
        "        \n",
        "          num_classes (int): number of input/output classes\n",
        "          emb_dim (int): token embedding size\n",
        "          hidden_dim (int): hidden layer size of RNNs\n",
        "          num_layers (int): number of RNN layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.input_layer = nn.Embedding(num_classes, emb_dim)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers)\n",
        "        self.out_layer = nn.Linear(hidden_dim, num_classes)\n",
        "        \n",
        "\n",
        "    def forward(self, input, state):\n",
        "      emb = self.input_layer(input)\n",
        "      output, state = self.lstm(emb, state)\n",
        "      output = self.out_layer(output)\n",
        "      output = output.view(-1, self.num_classes)\n",
        "      return output, state\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return weight.new_zeros(self.num_layers, bsz, self.hidden_dim), weight.new_zeros(self.num_layers, bsz, self.hidden_dim)\n",
        "\n",
        "\n",
        "# To be modified for LSTM...\n",
        "def custom_detach(h,c):\n",
        "    return h.detach(), c.detach()"
      ],
      "metadata": {
        "id": "MDOyDD_RkszQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoding"
      ],
      "metadata": {
        "id": "8ykt4fnYmRYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "@torch.no_grad()\n",
        "def complete(model, prompt, steps, sample):\n",
        "    \"\"\"Complete the prompt for as long as given steps using the model.\n",
        "    \n",
        "    Parameters:\n",
        "      model: language model\n",
        "      prompt (str): text segment to be completed\n",
        "      steps (int): number of decoding steps.\n",
        "      sample (bool): If True, sample from the model. Otherwise greedy.\n",
        "\n",
        "    Returns:\n",
        "      completed text (str)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    out_list = []\n",
        "    \n",
        "    # forward the prompt, compute prompt's ppl\n",
        "    prompt_list = []\n",
        "    char_prompt = list(prompt)\n",
        "    for char in char_prompt:\n",
        "        prompt_list.append(my_data.vocab.string_to_id[char])\n",
        "    x = torch.tensor(prompt_list).to(DEVICE).unsqueeze(1)\n",
        "    \n",
        "    states = model.init_hidden(1)\n",
        "    logits, states = model(x, states)\n",
        "    probs = F.softmax(logits[-1], dim=-1)\n",
        "    \n",
        "    # print(\"len of probs is: \", len(probs.cpu().numpy()))\n",
        "    length = len(probs.cpu().numpy())\n",
        "    # print(\"Props outside loop is: \", probs)\n",
        "\n",
        "    if sample:\n",
        "        ix = np.random.choice(len(probs.cpu().numpy()), size = 1, p = probs.cpu().numpy())\n",
        "        ix = torch.tensor(ix).to(DEVICE)\n",
        "        # print(\"ix is: \", ix)\n",
        "        out_list.append(my_data.vocab.id_to_string[int(ix)])\n",
        "        x = ix.unsqueeze(1)\n",
        "        # print(\"x is :\", x)\n",
        "\n",
        "    else:\n",
        "        max_p, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # print(\"max_p is\", max_p)\n",
        "        # print(\"props is: \", probs)\n",
        "        # print(\"ix is \", ix)\n",
        "\n",
        "        # print(\"ix is :\", ix)\n",
        "        out_list.append(my_data.vocab.id_to_string[int(ix)])\n",
        "        x = ix.unsqueeze(1)\n",
        "        # print(\"x is \", x)\n",
        "    \n",
        "    # decode \n",
        "    for k in range(steps):\n",
        "        logits, states = model(x, states)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        if sample:  # sample from the distribution or take the most likely\n",
        "            probs = probs.reshape(length)\n",
        "            ix = np.random.choice(len(probs.cpu().numpy()), size = 1, p = probs.cpu().numpy())\n",
        "            ix = torch.tensor(ix).to(DEVICE).unsqueeze(1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        out_list.append(my_data.vocab.id_to_string[int(ix)])\n",
        "        x = ix\n",
        "        # print(\"x is:\", x)\n",
        "    return ''.join(out_list)"
      ],
      "metadata": {
        "id": "1tgJH5LVkuB4"
      },
      "execution_count": 413,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "clipping = 1.0\n",
        "embedding_size = 64\n",
        "rnn_size = 2048\n",
        "rnn_num_layers = 1\n",
        "\n",
        "# vocab_size = len(module.vocab.itos)\n",
        "vocab_size = len(my_data.vocab.id_to_string)\n",
        "print(F\"vocab size: {vocab_size}\")\n",
        "\n",
        "model = LSTModel(\n",
        "    num_classes=vocab_size, emb_dim=embedding_size, hidden_dim=rnn_size,\n",
        "    num_layers=rnn_num_layers)\n",
        "model = model.to(DEVICE)\n",
        "hidden = model.init_hidden(batch_size)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tkrzgOmk1NK",
        "outputId": "cef009fa-a668-4189-8592-2325b4cf3b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop"
      ],
      "metadata": {
        "id": "jVcAiF_kmVWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "num_epochs = 30\n",
        "report_every = 30\n",
        "prompt = \"Dogs like best to\"\n",
        "\n",
        "# for question 2.1 \n",
        "perplexity = []\n",
        "total_loss = 0\n",
        "\n",
        "# for question 2.2\n",
        "nr_of_stages = 3\n",
        "stage1 = num_epochs/nr_of_stages              # Beginning\n",
        "stage2 = (num_epochs/nr_of_stages)+stage1     # Middle\n",
        "stage3 = (num_epochs/nr_of_stages)+stage2-1   # End \n",
        "\n",
        "text_out = []\n",
        "\n",
        "# For Question 3.1\n",
        "lowest_ppl = [1000, 0, 0]   # [ppl, epoch, idx]\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    print(f\"=== start epoch {ep} ===\")\n",
        "    state = model.init_hidden(batch_size)\n",
        "    # print(\"state is:\", state)\n",
        "    # for Question 3.1\n",
        "    h,c = state\n",
        "    for idx in range(len(batches)):\n",
        "        batch = batches[idx]\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        state = custom_detach(h,c)\n",
        "        input = batch[:-1]\n",
        "        target = batch[1:].reshape(-1)\n",
        "\n",
        "        bsz = input.shape[1]\n",
        "        prev_bsz = state[0].shape[1]\n",
        "        if bsz != prev_bsz:\n",
        "            state = state[0][:, :bsz, :], state[1][:, :bsz, :]\n",
        "        output, state = model(input, state)\n",
        "\n",
        "        # For question 2.1\n",
        "        loss = loss_fn(output, target)\n",
        "        # print(\"loss: \", loss)\n",
        "        ppl = math.exp(loss.item())\n",
        "        # print(\"ppl: \", ppl)\n",
        "\n",
        "        # For Question 3.2\n",
        "        if ppl < lowest_ppl[0]:\n",
        "          lowest_ppl[0] = ppl\n",
        "          lowest_ppl[1] = ep\n",
        "          lowest_ppl[2] = idx\n",
        "\n",
        "        perplexity.append(ppl)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
        "        optimizer.step()\n",
        "        if idx % report_every == 0:\n",
        "            print(f\"train ppl: {ppl}\")\n",
        "            generated_text = complete(model, prompt, 128, sample=False)\n",
        "            print(f'----------------- epoch/batch {ep}/{idx} -----------------')\n",
        "            print(prompt)\n",
        "            print(generated_text)\n",
        "\n",
        "            # For Question 2.2\n",
        "            if ep == (stage1 or stage2 or stage3) and idx == 0:\n",
        "              print(\"prompt is: \", prompt, \" text generated is: \", generated_text, \"ep is: \", ep, \" idx is: \", idx)\n",
        "              text_out.append(prompt + generated_text)\n",
        "            elif ep == stage2 and idx == 0:\n",
        "              print(\"prompt is: \", prompt, \" text generated is: \", generated_text, \"ep is: \", ep, \" idx is: \", idx)\n",
        "              text_out.append(prompt + generated_text)\n",
        "            elif ep == stage3 and idx == 0:\n",
        "              print(\"prompt is: \", prompt, \" text generated is: \", generated_text, \"ep is: \", ep, \" idx is: \", idx)\n",
        "              text_out.append(prompt + generated_text)\n",
        "\n",
        "            print(f'----------------- end generated text -------------------') \n",
        "\n",
        "# ______________________________________________________________________________\n",
        "# QUESTION 2.2 \n",
        "mean_perplexity = []\n",
        "total_ppl = 0\n",
        "steps = len(perplexity) / report_every\n",
        "for i in range(0,len(perplexity)):\n",
        "  total_ppl += perplexity[i]\n",
        "  if (i+1) % steps == 0:\n",
        "    ppl_mean = total_ppl / steps\n",
        "    mean_perplexity.append(ppl_mean)\n",
        "    total_ppl = 0\n",
        "\n",
        "fig, ppl = plt.subplots()\n",
        "ppl.plot(range(1, len(mean_perplexity)+1),mean_perplexity,\"-\")\n",
        "ppl.plot(range(1,len(mean_perplexity)+1), mean_perplexity, \"-\")\n",
        "ppl.set_xlabel(\"Iteration count\", fontsize = 16)\n",
        "ppl.set_ylabel(\"Perplexity\", fontsize = 16)\n",
        "ppl.set_title(f\"perplexity evolution of training\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "NGfdYEQYk212"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete(model, \"THE DONKEY IN THE LION’S SKIN\", 512, sample=False)"
      ],
      "metadata": {
        "id": "8U285u7ok5UY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "2a8a59ad-b645-4727-ad8a-f92fe259f2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\nA DONKEY, having started a Hare said, “I will at least go away a long time, the Wolf had followed the Sheep without attemping to the Project Gutenberg Literary Archive Foundation\\n\\nProject Gutenberg-tm depends (Letin). Illustrated by Copeland\\n\\n\\n\\n\\n                                                                                                                                                                                                                                                                        '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ______________________________________________________________________________\n",
        "# QUESTION 2.3\n",
        "print(complete(model, \"THE WOLF IN SHEEP’S CLOTHING\", 512, sample=False)) # title taken from the book"
      ],
      "metadata": {
        "id": "qzvyq557byWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ______________________________________________________________________________\n",
        "# QUESTION 2.3\n",
        "print(complete(model, \"THE WOLF DISGUISED AS HORSE\", 512, sample=False))   # made up title"
      ],
      "metadata": {
        "id": "S-xfqaqCbp0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ______________________________________________________________________________\n",
        "# QUESTION 2.3\n",
        "print(complete(model, \"A CROW ate a fruite of poisen\", 512, sample=False))   #  Some texts in a similar style."
      ],
      "metadata": {
        "id": "lYL7Hg-Gbpfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ______________________________________________________________________________\n",
        "# QUESTION 2.3\n",
        "print(complete(model, \"I was walking through the forest alone and all of a sudden\", 512, sample=False))   #  Anything I am interested in."
      ],
      "metadata": {
        "id": "aueps5LtcXR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ______________________________________________________________________________\n",
        "# QUESTION 3.4 (a)\n",
        "print(complete(model, \"THE THIRSTY PIGEON\", 512, sample=False))\n",
        "print(\"-\" * 200)\n",
        "print(complete(model, \"THE THIRSTY PIGEON\", 512, sample=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dl2RzBY9prM2",
        "outputId": "8e9b435a-d241-4e14-c6de-4571808ccba2"
      },
      "execution_count": 414,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "A PIGEON, preading his wagon through a miry lane, the wheels stuck fat their holes at his own image in the water that it attracted the honeycomb and at once threw down his ax. From that there was a storm. The terrible unseen wind came and struck to his play.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE WOLF AND THE HARE\n",
            "\n",
            "\n",
            "A WOLF made once spied a pitcher with a large sum of gold for distributing this eBook or online at finds and eat them, and the medium on which they may be stored, master, better nearly frozen.\n",
            "\n",
            "He took pity upon him and inv\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "S\n",
            "\n",
            "\n",
            "A PIGEON, preasing his just and all the first time the Flw young stopped in a tree that overhung the river and the jun and in danger of grain and hay ceased, and\n",
            "the Horse was drowned, and fly with beating he rad his way with some difficulty into a\n",
            "basket of corr. The Foundation makes no\n",
            "represent. The terms of this agreement for free              185\n",
            "    The Jak and the Elephant                           195\n",
            "    The Horse, the Crow and the Pitcher                 18\n",
            "    The Frogs who as he found the She\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ______________________________________________________________________________\n",
        "# QUESTION 3.4 (b)\n",
        "print(complete(model, \"THE WOLF SHALL NOT PASS\", 512, sample=False))\n",
        "print(\"-\" * 200)\n",
        "print(complete(model, \"THE WOLF SHALL NOT PASS\", 512, sample=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X-ebFLvqEd-",
        "outputId": "41451a7b-9036-4c06-9c75-804e2c416e22"
      },
      "execution_count": 415,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "A BOY once planted close beside a large and noble Pine Travess (Granged of the promise he had ridiculed.\n",
            "\n",
            "The Foundation's principal office is in later times, ran at once the drover to pass by saw him there, and stopped to look at them.\n",
            "\n",
            "“What a foolish creature you were, to look so sour and so for you, and you shall be found in the act of this work or any one eight should deep, she made a little journey\n",
            "together.\n",
            "\n",
            "When night, he saw the cuckoo to the rank of lenstry friend, who, amazed at the good things\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "A CHARCOAL Burner carried one day tall at last agreed to donate royalting better than that. Let us hang a bell about his neck and crammed his Son to gather an abundant supply of food—enough to be willing to see which you comply with all other teaching is percians, and I have let him can in the f.ole of sticks tied\n",
            "tightly treest to the ground, and its head was a\n",
            "reasonable after as they were; but as soon as the morning day, the Satyr pounced upon a pot of honey, which was so kindly\n",
            "taken that he bade one \n"
          ]
        }
      ]
    }
  ]
}